{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB3Ab90aNOFm"
      },
      "source": [
        "Copyright (c) MONAI Consortium  \n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
        "you may not use this file except in compliance with the License.  \n",
        "You may obtain a copy of the License at  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
        "Unless required by applicable law or agreed to in writing, software  \n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
        "See the License for the specific language governing permissions and  \n",
        "limitations under the License.\n",
        "\n",
        "# Fast training with MONAI features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvNzw8ybNOFn"
      },
      "source": [
        "This tutorial shows a regular PyTorch training program and a MONAI optimized training program, and compared the performance.  \n",
        "Mainly includes:\n",
        "1. AMP (Auto mixed precision).\n",
        "2. CacheDataset for deterministic transforms.\n",
        "3. Move data to GPU and cache, then execute random transforms on GPU.\n",
        "4. Disable meta tracking in the random transforms to avoid unnecessary computation.\n",
        "5. multi-threads `ThreadDataLoader` is faster than PyTorch DataLoader in light-weight task.\n",
        "6. Use MONAI `DiceCE` loss instead of regular `Dice` loss.\n",
        "7. Analyzed training curve and tuned algorithm: Use `SGD` optimizer, different network parameters, etc.\n",
        "\n",
        "With a A100 GPU and the target validation `mean dice = 0.94` of the `forground` channel only,  it's more than `150x` speedup compared with the Pytorch regular implementation when achieving the same metric. And every epoch is more than `50x` faster than regular training.\n",
        "\n",
        "It's modified from the Spleen 3D segmentation tutorial notebook, the Spleen dataset can be downloaded from http://medicaldecathlon.com/.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/acceleration/fast_training_tutorial.ipynb)(* please note that the free GPU resource in Colab may be not as powerful as the A100 test results in this notebook: it may not support AMP and the GPU computation of transforms may be not faster than the CPU computation.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEJT3EjnNOFn"
      },
      "source": [
        "## Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44d_r17BNOFo",
        "outputId": "de5ec466-90c6-433d-8ea9-002bc454d423"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "ModuleNotFoundError: No module named 'monai'\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\"\n",
        "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95P0L_znNOFo"
      },
      "source": [
        "## Setup imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7sWhk-qZNOFo",
        "outputId": "09a1c6ad-adb1-44c8-8da4-48f442577716",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MONAI version: 1.6.dev2544\n",
            "Numpy version: 2.0.2\n",
            "Pytorch version: 2.8.0+cu126\n",
            "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
            "MONAI rev id: f910be902091ce9efdc2928e1eb473fd607bfeb9\n",
            "MONAI __file__: /usr/local/lib/python3.12/dist-packages/monai/__init__.py\n",
            "\n",
            "Optional dependencies:\n",
            "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "Nibabel version: 5.3.2\n",
            "scikit-image version: 0.25.2\n",
            "scipy version: 1.16.3\n",
            "Pillow version: 11.3.0\n",
            "Tensorboard version: 2.19.0\n",
            "gdown version: 5.2.0\n",
            "TorchVision version: 0.23.0+cu126\n",
            "tqdm version: 4.67.1\n",
            "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "psutil version: 5.9.5\n",
            "pandas version: 2.2.2\n",
            "einops version: 0.8.1\n",
            "transformers version: 4.57.1\n",
            "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "\n",
            "For details about installing the optional dependencies, please visit:\n",
            "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import math\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.optim import Adam, SGD\n",
        "from monai.apps import download_and_extract\n",
        "from monai.config import print_config\n",
        "from monai.data import (\n",
        "    CacheDataset,\n",
        "    DataLoader,\n",
        "    ThreadDataLoader,\n",
        "    Dataset,\n",
        "    decollate_batch,\n",
        "    set_track_meta,\n",
        ")\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.losses import DiceLoss, DiceCELoss\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.networks.layers import Act, Norm\n",
        "from monai.networks.nets import UNet\n",
        "from monai.transforms import (\n",
        "    EnsureChannelFirstd,\n",
        "    AsDiscrete,\n",
        "    Compose,\n",
        "    CropForegroundd,\n",
        "    EnsureTyped,\n",
        "    FgBgToIndicesd,\n",
        "    LoadImaged,\n",
        "    Orientationd,\n",
        "    RandCropByPosNegLabeld,\n",
        "    ScaleIntensityRanged,\n",
        "    Spacingd,\n",
        ")\n",
        "from monai.utils import set_determinism\n",
        "\n",
        "# for profiling\n",
        "import nvtx\n",
        "from monai.utils.nvtx import Range\n",
        "import contextlib  # to improve code readability (combining training/validation loop with and without profiling)\n",
        "\n",
        "print_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8gDSzKoNOFo"
      },
      "source": [
        "## Setup data & output directories\n",
        "\n",
        "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \n",
        "This allows you to save results and reuse downloads.  \n",
        "If not specified a temporary directory will be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "r4mGga53NOFp",
        "outputId": "ec93b2d9-9ce4-4d84-829e-ccca0be01a70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root dir is: /tmp/tmp9dmry_0e\n"
          ]
        }
      ],
      "source": [
        "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
        "if directory is not None:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
        "print(f\"root dir is: {root_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8kx2qE8NOFp"
      },
      "source": [
        "By default, outputs will go to `outputs/`.\n",
        "\n",
        "You can run this tutorial twice, once with profiling and once without, and the outputs will not conflict with each other.\n",
        "- When profiling, the output is `outputs/output_base.nsys-rep`, which you can then visualize using the GUI of Nsight systems (a brief guide is provided in the \"Profiling visualization\" section below).\n",
        "- When not profiling, the outputs are `outputs/loss_dice_comparison.png`, `outputs/metric_time_epochs.png`, and `outputs/total_epoch_time_comparison.png`.\n",
        "\n",
        "We set up the tutorial such that figures are only generated when not profiling, but that does not have to be the case. In general, the figures make more sense when training is run for a higher number of epochs (e.g., hundreds), which is usually not the case when profiling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YYooIQI0NOFp"
      },
      "outputs": [],
      "source": [
        "# outputs\n",
        "\n",
        "out_dir = \"outputs/\"\n",
        "\n",
        "if not os.path.exists(out_dir):\n",
        "    os.makedirs(out_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvvIUJG0NOFp"
      },
      "source": [
        "## Profiling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HIqxqPmNOFp"
      },
      "source": [
        "This section sets up profiling for this tutorial.\n",
        "\n",
        "The number of epochs is automatically set based on whether profiling is being performed, but you can modify as needed.\n",
        "\n",
        "- If you are not interested in profiling, please set `profiling = False` and move on.\n",
        "\n",
        "- If you are profiling:\n",
        "\n",
        "  - Because of the currently supported functionality of Nsight systems (`nsys`), profiling can only be performed from the terminal, and not from within this tutorial. For more information, including installation, refer to the [NVIDIA Nsight Systems page](https://developer.nvidia.com/nsight-systems).\n",
        "  - Perform the following steps:\n",
        "  \n",
        "    1) Make sure `nsys` is installed;\n",
        "    \n",
        "    2) Set `profiling = True`;\n",
        "    \n",
        "    3) Make sure all lines in \"Setup environment\" (first code cell in this tutorial, above) are commented out;\n",
        "    \n",
        "    4) Save this notebook;\n",
        "    \n",
        "    5) Open the terminal and ensure that you are in the directory of this notebook, then run this command:\n",
        "    `jupyter nbconvert fast_training_tutorial.ipynb --to python && nsys profile --output ./outputs/output_base --force-overwrite true --trace-fork-before-exec true python3 fast_training_tutorial.py ; rm fast_training_tutorial.py`\n",
        "    \n",
        "    This command converts the notebook to a Python script locally and runs `nsys`. The output file is `outputs/output_base.nsys-rep`, but you can modify `--output` to specify the desired location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "D12x3k6jNOFq"
      },
      "outputs": [],
      "source": [
        "profiling = False\n",
        "\n",
        "# to see the trend in training curve and dice results, set max_epochs to be larger (300)\n",
        "# note that before optimization, training can be quite a bit slower\n",
        "if profiling:\n",
        "    max_epochs = 6\n",
        "else:\n",
        "    max_epochs = 300\n",
        "\n",
        "# to improve readability\n",
        "\n",
        "\n",
        "def range_func(x, y):\n",
        "    return Range(x)(y) if profiling else y\n",
        "\n",
        "\n",
        "no_profiling = contextlib.nullcontext()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OvuvhvCNOFq"
      },
      "source": [
        "## Download dataset\n",
        "\n",
        "Downloads and extracts the Decathlon Spleen dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NAK1mQ2nNOFq"
      },
      "outputs": [],
      "source": [
        "resource = \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task09_Spleen.tar\"\n",
        "md5 = \"410d4a301da4e5b2f6f86ec3ddba524e\"\n",
        "\n",
        "compressed_file = os.path.join(root_dir, \"Task09_Spleen.tar\")\n",
        "data_root = os.path.join(root_dir, \"Task09_Spleen\")\n",
        "if not os.path.exists(data_root):\n",
        "    download_and_extract(resource, compressed_file, root_dir, md5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BtqW4PzNOFq"
      },
      "source": [
        "## Set MSD Spleen dataset path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JQumfXurNOFq"
      },
      "outputs": [],
      "source": [
        "train_images = sorted(glob.glob(os.path.join(data_root, \"imagesTr\", \"*.nii.gz\")))\n",
        "train_labels = sorted(glob.glob(os.path.join(data_root, \"labelsTr\", \"*.nii.gz\")))\n",
        "data_dicts = [{\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(train_images, train_labels)]\n",
        "train_files, val_files = data_dicts[:-9], data_dicts[-9:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDj2w4-WNOFq"
      },
      "source": [
        "## Setup transforms for training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "TlhvJv4SNOFq"
      },
      "outputs": [],
      "source": [
        "def transformations(fast=False, device=\"cuda:0\"):\n",
        "    train_transforms = [\n",
        "        range_func(\"LoadImage\", LoadImaged(keys=[\"image\", \"label\"])),\n",
        "        range_func(\"EnsureChannelFirst\", EnsureChannelFirstd(keys=[\"image\", \"label\"])),\n",
        "        range_func(\"Orientation\", Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\")),\n",
        "        range_func(\n",
        "            \"Spacing\",\n",
        "            Spacingd(\n",
        "                keys=[\"image\", \"label\"],\n",
        "                pixdim=(1.5, 1.5, 2.0),\n",
        "                mode=(\"bilinear\", \"nearest\"),\n",
        "            ),\n",
        "        ),\n",
        "        range_func(\n",
        "            \"ScaleIntensityRange\",\n",
        "            ScaleIntensityRanged(\n",
        "                keys=[\"image\"],\n",
        "                a_min=-57,\n",
        "                a_max=164,\n",
        "                b_min=0.0,\n",
        "                b_max=1.0,\n",
        "                clip=True,\n",
        "            ),\n",
        "        ),\n",
        "        range_func(\"CropForeground\", CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\", allow_smaller=True)),\n",
        "        # pre-compute foreground and background indexes\n",
        "        # and cache them to accelerate training\n",
        "        range_func(\n",
        "            \"Indexing\",\n",
        "            FgBgToIndicesd(\n",
        "                keys=\"label\",\n",
        "                fg_postfix=\"_fg\",\n",
        "                bg_postfix=\"_bg\",\n",
        "                image_key=\"image\",\n",
        "            ),\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    if fast:\n",
        "        # convert the data to Tensor without meta, move to GPU and cache to avoid CPU -> GPU sync in every epoch\n",
        "        train_transforms.append(\n",
        "            range_func(\"EnsureType\", EnsureTyped(keys=[\"image\", \"label\"], device=device, track_meta=False))\n",
        "        )\n",
        "\n",
        "    train_transforms.append(\n",
        "        # randomly crop out patch samples from big\n",
        "        # image based on pos / neg ratio\n",
        "        # the image centers of negative samples\n",
        "        # must be in valid image area\n",
        "        range_func(\n",
        "            \"RandCrop\",\n",
        "            RandCropByPosNegLabeld(\n",
        "                keys=[\"image\", \"label\"],\n",
        "                label_key=\"label\",\n",
        "                spatial_size=(96, 96, 96),\n",
        "                pos=1,\n",
        "                neg=1,\n",
        "                num_samples=4,\n",
        "                fg_indices_key=\"label_fg\",\n",
        "                bg_indices_key=\"label_bg\",\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    val_transforms = [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            pixdim=(1.5, 1.5, 2.0),\n",
        "            mode=(\"bilinear\", \"nearest\"),\n",
        "        ),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"],\n",
        "            a_min=-57,\n",
        "            a_max=164,\n",
        "            b_min=0.0,\n",
        "            b_max=1.0,\n",
        "            clip=True,\n",
        "        ),\n",
        "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\", allow_smaller=True),\n",
        "    ]\n",
        "    if fast:\n",
        "        # convert the data to Tensor without meta, move to GPU and cache to avoid CPU -> GPU sync in every epoch\n",
        "        val_transforms.append(EnsureTyped(keys=[\"image\", \"label\"], device=device, track_meta=False))\n",
        "\n",
        "    return Compose(train_transforms), Compose(val_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAj2wXh2NOFq"
      },
      "source": [
        "## Define the training progress\n",
        "For a typical PyTorch regular training procedure, use regular `Dataset`, `DataLoader`, `Adam` optimizer and `Dice` loss to train the model.\n",
        "\n",
        "For MONAI fast training progress, we mainly introduce the following features:\n",
        "1. `AMP` (auto mixed precision): AMP is an important feature released in PyTorch v1.6, NVIDIA CUDA 11 added strong support for AMP and significantly improved training speed.\n",
        "2. `CacheDataset`: Dataset with the cache mechanism that can load data and cache deterministic transforms' result during training.\n",
        "3. `EnsureTyped` transform: to move data to GPU and cache with `CacheDataset`, then execute random transforms on GPU directly, avoid CPU -> GPU sync in every epoch. Please note that not all the MONAI transforms support GPU operation so far, still working in progress.\n",
        "4. `set_track_meta(False)`: to disable meta tracking in the random transforms to avoid unnecessary computation.\n",
        "5. `ThreadDataLoader`: uses multi-threads instead of multi-processing, faster than `DataLoader` in light-weight task as we already cached the results of most computation.\n",
        "6. `DiceCE` loss function: computes Dice loss and Cross Entropy Loss, returns the weighted sum of these two losses.\n",
        "7. Analyzed the training curve and tuned algorithm: Use `SGD` optimizer, different network parameters, etc.\n",
        "\n",
        "(A note on code: to improve readability and support the profiling flag, we used the `with nvtx(...) if profiling else no_profiling` context pattern, where `no_profiling` is a null context from Python's native `contextlib` with no effect on the code. An acknowledgement is provided here[<sup id=\"fn1-back\">1</sup>](#fn1).)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "b2dU4FfrNOFq"
      },
      "outputs": [],
      "source": [
        "def train_process(fast=False):\n",
        "    learning_rate = 2e-4\n",
        "    val_interval = 5  # do validation for every epoch\n",
        "    set_track_meta(True)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda:0\")\n",
        "    else:\n",
        "        raise RuntimeError(\"this tutorial is intended for GPU, but no CUDA device is available\")\n",
        "\n",
        "    train_trans, val_trans = transformations(fast=fast, device=device)\n",
        "    # set CacheDataset, ThreadDataLoader and DiceCE loss for MONAI fast training\n",
        "    if fast:\n",
        "        # as `RandCropByPosNegLabeld` crops from the cached content and `deepcopy`\n",
        "        # the crop area instead of modifying the cached value, we can set `copy_cache=False`\n",
        "        # to avoid unnecessary deepcopy of cached content in `CacheDataset`\n",
        "        train_ds = CacheDataset(\n",
        "            data=train_files,\n",
        "            transform=train_trans,\n",
        "            cache_rate=1.0,\n",
        "            num_workers=8,\n",
        "            copy_cache=False,\n",
        "        )\n",
        "        val_ds = CacheDataset(data=val_files, transform=val_trans, cache_rate=1.0, num_workers=5, copy_cache=False)\n",
        "        # disable multi-workers because `ThreadDataLoader` works with multi-threads\n",
        "        train_loader = ThreadDataLoader(train_ds, num_workers=0, batch_size=4, shuffle=True)\n",
        "        val_loader = ThreadDataLoader(val_ds, num_workers=0, batch_size=1)\n",
        "\n",
        "        loss_function = DiceCELoss(\n",
        "            include_background=False,\n",
        "            to_onehot_y=True,\n",
        "            softmax=True,\n",
        "            squared_pred=True,\n",
        "            batch=True,\n",
        "            smooth_nr=0.00001,\n",
        "            smooth_dr=0.00001,\n",
        "            lambda_dice=0.5,\n",
        "            lambda_ce=0.5,\n",
        "        )\n",
        "        model = UNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=1,\n",
        "            out_channels=2,\n",
        "            channels=(32, 64, 128, 256, 512),\n",
        "            strides=(2, 2, 2, 2),\n",
        "            num_res_units=2,\n",
        "            norm=Norm.BATCH,\n",
        "            kernel_size=3,\n",
        "            up_kernel_size=3,\n",
        "            act=Act.PRELU,\n",
        "            dropout=0.2,\n",
        "            bias=True,\n",
        "        ).to(device)\n",
        "        # avoid the computation of meta information in random transforms\n",
        "        set_track_meta(False)\n",
        "    else:\n",
        "        train_ds = Dataset(data=train_files, transform=train_trans)\n",
        "        val_ds = Dataset(data=val_files, transform=val_trans)\n",
        "        train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=8)\n",
        "        val_loader = DataLoader(val_ds, batch_size=1, num_workers=4)\n",
        "        loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
        "        model = UNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=1,\n",
        "            out_channels=2,\n",
        "            channels=(16, 32, 64, 128, 256),\n",
        "            strides=(2, 2, 2, 2),\n",
        "            num_res_units=2,\n",
        "            norm=Norm.BATCH,\n",
        "        ).to(device)\n",
        "\n",
        "    post_pred = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
        "    post_label = Compose([AsDiscrete(to_onehot=2)])\n",
        "\n",
        "    dice_metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
        "\n",
        "    if fast:\n",
        "        # SGD prefer to much bigger learning rate\n",
        "        optimizer = SGD(\n",
        "            model.parameters(),\n",
        "            lr=learning_rate * 1000,\n",
        "            momentum=0.9,\n",
        "            weight_decay=0.00004,\n",
        "        )\n",
        "        scaler = torch.GradScaler(\"cuda\")\n",
        "    else:\n",
        "        optimizer = Adam(model.parameters(), learning_rate)\n",
        "\n",
        "    best_metric = -1\n",
        "    best_metric_epoch = -1\n",
        "    best_metrics_epochs_and_time = [[], [], []]\n",
        "    epoch_loss_values = []\n",
        "    metric_values = []\n",
        "    epoch_times = []\n",
        "    total_start = time.time()\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(\"-\" * 10)\n",
        "        print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
        "\n",
        "        # profiling: full epoch\n",
        "        with nvtx.annotate(\"epoch\", color=\"red\") if profiling else no_profiling:\n",
        "            model.train()\n",
        "            epoch_loss = 0\n",
        "            train_loader_iterator = iter(train_loader)\n",
        "\n",
        "            # using step instead of iterate through train_loader directly to track data loading time\n",
        "            # steps are 1-indexed for printing and calculation purposes\n",
        "            for step in range(1, len(train_loader) + 1):\n",
        "                step_start = time.time()\n",
        "\n",
        "                # profiling: train dataload\n",
        "                with nvtx.annotate(\"dataload\", color=\"red\") if profiling else no_profiling:\n",
        "                    # rng_train_dataload = nvtx.start_range(message=\"dataload\", color=\"red\")\n",
        "                    batch_data = next(train_loader_iterator)\n",
        "                    inputs, labels = (\n",
        "                        batch_data[\"image\"].to(device),\n",
        "                        batch_data[\"label\"].to(device),\n",
        "                    )\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                # set AMP for MONAI training\n",
        "                if fast:\n",
        "                    # profiling: forward\n",
        "                    with nvtx.annotate(\"forward\", color=\"green\") if profiling else no_profiling:\n",
        "                        with torch.autocast(\"cuda\"):\n",
        "                            outputs = model(inputs)\n",
        "                            loss = loss_function(outputs, labels)\n",
        "\n",
        "                    # profiling: backward\n",
        "                    with nvtx.annotate(\"backward\", color=\"blue\") if profiling else no_profiling:\n",
        "                        scaler.scale(loss).backward()\n",
        "\n",
        "                    # profiling: update\n",
        "                    with nvtx.annotate(\"update\", color=\"yellow\") if profiling else no_profiling:\n",
        "                        scaler.step(optimizer)\n",
        "                        scaler.update()\n",
        "                else:\n",
        "                    # profiling: forward\n",
        "                    with nvtx.annotate(\"forward\", color=\"green\") if profiling else no_profiling:\n",
        "                        outputs = model(inputs)\n",
        "                        loss = loss_function(outputs, labels)\n",
        "\n",
        "                    # profiling: backward\n",
        "                    with nvtx.annotate(\"backward\", color=\"blue\") if profiling else no_profiling:\n",
        "                        loss.backward()\n",
        "\n",
        "                    # profiling: update\n",
        "                    with nvtx.annotate(\"update\", color=\"yellow\") if profiling else no_profiling:\n",
        "                        optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                epoch_len = math.ceil(len(train_ds) / train_loader.batch_size)\n",
        "                print(\n",
        "                    f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\" f\" step time: {(time.time() - step_start):.4f}\"\n",
        "                )\n",
        "            epoch_loss /= step\n",
        "            epoch_loss_values.append(epoch_loss)\n",
        "            print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
        "\n",
        "            if (epoch + 1) % val_interval == 0:\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    val_loader_iterator = iter(val_loader)\n",
        "\n",
        "                    for _ in range(len(val_loader)):\n",
        "                        # profiling: val dataload\n",
        "                        with nvtx.annotate(\"dataload\", color=\"red\") if profiling else no_profiling:\n",
        "                            val_data = next(val_loader_iterator)\n",
        "                            val_inputs, val_labels = (\n",
        "                                val_data[\"image\"].to(device),\n",
        "                                val_data[\"label\"].to(device),\n",
        "                            )\n",
        "\n",
        "                        roi_size = (160, 160, 160)\n",
        "                        sw_batch_size = 4\n",
        "\n",
        "                        # profiling: sliding window\n",
        "                        with nvtx.annotate(\"sliding window\", color=\"green\") if profiling else no_profiling:\n",
        "                            # set AMP for MONAI validation\n",
        "                            if fast:\n",
        "                                with torch.autocast(\"cuda\"):\n",
        "                                    val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model)\n",
        "                            else:\n",
        "                                val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model)\n",
        "\n",
        "                        # profiling: decollate batch\n",
        "                        with nvtx.annotate(\"decollate batch\", color=\"blue\") if profiling else no_profiling:\n",
        "                            val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
        "                            val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
        "\n",
        "                        # profiling: compute metric\n",
        "                        with nvtx.annotate(\"compute metric\", color=\"yellow\") if profiling else no_profiling:\n",
        "                            dice_metric(y_pred=val_outputs, y=val_labels)\n",
        "\n",
        "                    metric = dice_metric.aggregate().item()\n",
        "                    dice_metric.reset()\n",
        "                    metric_values.append(metric)\n",
        "                    if metric > best_metric:\n",
        "                        best_metric = metric\n",
        "                        best_metric_epoch = epoch + 1\n",
        "                        best_metrics_epochs_and_time[0].append(best_metric)\n",
        "                        best_metrics_epochs_and_time[1].append(best_metric_epoch)\n",
        "                        best_metrics_epochs_and_time[2].append(time.time() - total_start)\n",
        "                        torch.save(model.state_dict(), os.path.join(root_dir, \"best_metric_model.pt\"))\n",
        "                        print(\"saved new best metric model\")\n",
        "                    print(\n",
        "                        f\"current epoch: {epoch + 1} current\"\n",
        "                        f\" mean dice: {metric:.4f}\"\n",
        "                        f\" best mean dice: {best_metric:.4f}\"\n",
        "                        f\" at epoch: {best_metric_epoch}\"\n",
        "                    )\n",
        "        print(f\"time consuming of epoch {epoch + 1} is:\" f\" {(time.time() - epoch_start):.4f}\")\n",
        "        epoch_times.append(time.time() - epoch_start)\n",
        "\n",
        "    total_time = time.time() - total_start\n",
        "    print(\n",
        "        f\"train completed, best_metric: {best_metric:.4f}\"\n",
        "        f\" at epoch: {best_metric_epoch}\"\n",
        "        f\" total time: {total_time:.4f}\"\n",
        "    )\n",
        "    return (\n",
        "        max_epochs,\n",
        "        epoch_loss_values,\n",
        "        metric_values,\n",
        "        epoch_times,\n",
        "        best_metrics_epochs_and_time,\n",
        "        total_time,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsBYWQKWNOFr"
      },
      "source": [
        "## Enable determinism and execute regular PyTorch training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "scrolled": true,
        "id": "KrY_ib0fNOFr",
        "outputId": "78b1ddf3-82c1-4019-9c30-7025ce2b7a92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "this tutorial is intended for GPU, but no CUDA device is available",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-259274434.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_time\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m ) = train_process(fast=False)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtotal_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mregular_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"total time of {epoch_num} epochs with regular PyTorch training: {total_time:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3944021197.py\u001b[0m in \u001b[0;36mtrain_process\u001b[0;34m(fast)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"this tutorial is intended for GPU, but no CUDA device is available\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrain_trans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: this tutorial is intended for GPU, but no CUDA device is available"
          ]
        }
      ],
      "source": [
        "set_determinism(seed=0)\n",
        "regular_start = time.time()\n",
        "(\n",
        "    epoch_num,\n",
        "    epoch_loss_values,\n",
        "    metric_values,\n",
        "    epoch_times,\n",
        "    best,\n",
        "    train_time,\n",
        ") = train_process(fast=False)\n",
        "total_time = time.time() - regular_start\n",
        "print(f\"total time of {epoch_num} epochs with regular PyTorch training: {total_time:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWdgoDQTNOFr"
      },
      "source": [
        "## Enable determinism and execute MONAI optimized training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "O7kdLmjANOFr",
        "outputId": "18b23e3e-c71c-4c19-fd26-0f2af73a5a45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "this tutorial is intended for GPU, but no CUDA device is available",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3634811525.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mm_best\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mm_train_time\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m ) = train_process(fast=True)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mm_total_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmonai_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m print(\n",
            "\u001b[0;32m/tmp/ipython-input-3944021197.py\u001b[0m in \u001b[0;36mtrain_process\u001b[0;34m(fast)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"this tutorial is intended for GPU, but no CUDA device is available\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrain_trans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: this tutorial is intended for GPU, but no CUDA device is available"
          ]
        }
      ],
      "source": [
        "set_determinism(seed=0)\n",
        "monai_start = time.time()\n",
        "(\n",
        "    epoch_num,\n",
        "    m_epoch_loss_values,\n",
        "    m_metric_values,\n",
        "    m_epoch_times,\n",
        "    m_best,\n",
        "    m_train_time,\n",
        ") = train_process(fast=True)\n",
        "m_total_time = time.time() - monai_start\n",
        "print(\n",
        "    f\"total time of {epoch_num} epochs with MONAI fast training: {m_train_time:.4f},\"\n",
        "    f\" time of preparing cache: {(m_total_time - m_train_time):.4f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REOryIfQNOFr"
      },
      "source": [
        "## Profiling visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsqdDm4-NOFr"
      },
      "source": [
        "Here we give a brief overview of key observations from the `nsys-rep` output file when opened in Nsight systems (2022.2.1).\n",
        "\n",
        "In the GUI, select File -> Open -> output_base.nsys-rep. Here is a sample of the display:\n",
        "\n",
        "![png](https://github.com/Project-MONAI/tutorials/blob/main/acceleration/figures/nsys-all-annotated.png?raw=1)\n",
        "- To get a better view of details, you can left-click and select a horizontal section, then right-click and \"Zoom into Selection.\" To return, right-click and select \"Reset Zoom.\"\n",
        "- Sections B and C show training before and after acceleration (when fast=False and fast=True, accordingly). Clearly, MONAI optimized training is much faster than regular PyTorch training. B and C both contain two rows; the upper row shows per-epoch time, and the lower row shows per-action time (user-defined, like dataloading, forward, backward, etc.).\n",
        "- Section A shows GPU utilization, where the height of the blue bars represents utilization rate. Regular PyTorch training shows sporadic and varying levels of GPU utilization, while MONAI optimized training shows consistent and high levels of GPU utilization.\n",
        "\n",
        "Expanding one more thread in the lower left corner and several more threads below \\[4648\\], we see the following:\n",
        "\n",
        "![png](https://github.com/Project-MONAI/tutorials/blob/main/acceleration/figures/nsys-transforms-annotated.png?raw=1)\n",
        "\n",
        "Sections D and E both include information on the transform chain.\n",
        "- Section E: In MONAI optimized training, results of all transforms in the chain until the first randomized transform is stored to prevent repeated operations. This explains why E is chronologically before any of the training epochs in the figure.\n",
        "- Section D: In regular PyTorch training, CacheDataset is not in use, and the transform chain is performed every epoch on all data used.\n",
        "\n",
        "Here is the display of the transform chain when zoomed in:\n",
        "![png](https://github.com/Project-MONAI/tutorials/blob/main/acceleration/figures/nsys-fast-transform.png?raw=1)\n",
        "\n",
        "And a display of one training epoch of MONAI optimized training when zoomed in:\n",
        "![png](https://github.com/Project-MONAI/tutorials/blob/main/acceleration/figures/nsys-epoch-short.png?raw=1)\n",
        "\n",
        "Notice that the per-epoch time is >20 times faster than the regular PyTorch training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qyUiZH7NOFr"
      },
      "source": [
        "## Plot training loss and validation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aza1f0INOFr"
      },
      "outputs": [],
      "source": [
        "if not profiling:\n",
        "    plt.figure(\"train\", (12, 12))\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.title(\"Regular Epoch Average Loss\")\n",
        "    x = [i + 1 for i in range(len(epoch_loss_values))]\n",
        "    y = epoch_loss_values\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.grid(alpha=0.4, linestyle=\":\")\n",
        "    plt.plot(x, y, color=\"red\")\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.title(\"Regular Val Mean Dice\")\n",
        "    x = [(i + 1) * 5 for i in range(len(metric_values))]\n",
        "    y = metric_values\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.grid(alpha=0.4, linestyle=\":\")\n",
        "    plt.plot(x, y, color=\"red\")\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.title(\"Fast Epoch Average Loss\")\n",
        "    x = [i + 1 for i in range(len(m_epoch_loss_values))]\n",
        "    y = m_epoch_loss_values\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.grid(alpha=0.4, linestyle=\":\")\n",
        "    plt.plot(x, y, color=\"green\")\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.title(\"Fast Val Mean Dice\")\n",
        "    x = [(i + 1) * 5 for i in range(len(m_metric_values))]\n",
        "    y = m_metric_values\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.grid(alpha=0.4, linestyle=\":\")\n",
        "    plt.plot(x, y, color=\"green\")\n",
        "    plt.savefig(\"outputs/loss_dice_comparison.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4h8xSP7NOFs"
      },
      "source": [
        "## Plot total time and every epoch time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n913rnbCNOFs"
      },
      "outputs": [],
      "source": [
        "if not profiling:\n",
        "    plt.figure(\"train\", (12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(\"Total Train Time(300 epochs)\")\n",
        "    plt.bar(\"regular PyTorch\", total_time, 1, label=\"Regular training\", color=\"red\")\n",
        "    plt.bar(\"Fast\", m_total_time, 1, label=\"Fast training\", color=\"green\")\n",
        "    plt.ylabel(\"secs\")\n",
        "    plt.grid(alpha=0.4, linestyle=\":\")\n",
        "    plt.legend(loc=\"best\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(\"Epoch Time\")\n",
        "    x = [i + 1 for i in range(len(epoch_times))]\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"secs\")\n",
        "    plt.plot(x, epoch_times, label=\"Regular training\", color=\"red\")\n",
        "    plt.plot(x, m_epoch_times, label=\"Fast training\", color=\"green\")\n",
        "    plt.grid(alpha=0.4, linestyle=\":\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.savefig(\"outputs/total_epoch_time_comparison.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyYTRelENOFs"
      },
      "source": [
        "## Plot total time to achieve metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EE35ZGydNOFs"
      },
      "outputs": [],
      "source": [
        "def get_best_metric_time(threshold, best_values):\n",
        "    for i, v in enumerate(best_values[0]):\n",
        "        if round(v, 4) >= threshold:\n",
        "            return best_values[2][i]\n",
        "    return -1\n",
        "\n",
        "\n",
        "def get_best_metric_epochs(threshold, best_values):\n",
        "    for i, v in enumerate(best_values[0]):\n",
        "        if round(v, 4) >= threshold:\n",
        "            return best_values[1][i]\n",
        "    return -1\n",
        "\n",
        "\n",
        "def get_label(index):\n",
        "    if index == 0:\n",
        "        return \"Regular training\"\n",
        "    elif index == 1:\n",
        "        return \"Fast training\"\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "if not profiling:\n",
        "    plt.set_loglevel(\"WARNING\")\n",
        "\n",
        "    plt.figure(\"train\", (18, 6))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title(\"Metrics Time\")\n",
        "    plt.xlabel(\"secs\")\n",
        "    plt.ylabel(\"best mean_dice\")\n",
        "    plt.plot(best[2], best[0], label=\"Regular training\", color=\"red\")\n",
        "    plt.plot(m_best[2], m_best[0], label=\"Fast training\", color=\"green\")\n",
        "    plt.grid(alpha=0.4, linestyle=\":\")\n",
        "    plt.legend(loc=\"best\")\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.title(\"Typical Metrics Time\")\n",
        "    plt.xlabel(\"best mean_dice\")\n",
        "    plt.ylabel(\"secs\")\n",
        "    labels = [\"0.80\", \"0.80 \", \"0.90\", \"0.90 \", \"0.92\", \"0.92 \", \"0.94\", \"0.94 \"]\n",
        "    x_values = [0.8, 0.8, 0.9, 0.9, 0.92, 0.92, 0.94, 0.94]\n",
        "    for i, (l, x) in enumerate(zip(labels, x_values)):\n",
        "        value = int(get_best_metric_time(x, best if i % 2 == 0 else m_best))\n",
        "        color = \"red\" if i % 2 == 0 else \"green\"\n",
        "        plt.bar(l, value, 0.5, label=get_label(i), color=color)\n",
        "        plt.text(l, value, \"%s\" % value, ha=\"center\", va=\"bottom\")\n",
        "    plt.grid(alpha=0.4, linestyle=\":\")\n",
        "    plt.legend(loc=\"best\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.title(\"Typical Metrics Epochs\")\n",
        "    plt.xlabel(\"best mean_dice\")\n",
        "    plt.ylabel(\"epochs\")\n",
        "    for i, (l, x) in enumerate(zip(labels, x_values)):\n",
        "        value = int(get_best_metric_epochs(x, best if i % 2 == 0 else m_best))\n",
        "        color = \"red\" if i % 2 == 0 else \"green\"\n",
        "        plt.bar(l, value, 0.5, label=get_label(i), color=color)\n",
        "        plt.text(l, value, \"%s\" % value, ha=\"center\", va=\"bottom\")\n",
        "    plt.grid(alpha=0.4, linestyle=\":\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.savefig(\"outputs/metric_time_epochs.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJRd78T1NOFs"
      },
      "source": [
        "## Cleanup data directory\n",
        "\n",
        "Remove directory if a temporary was used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNZb6XrrNOFs"
      },
      "outputs": [],
      "source": [
        "if directory is None:\n",
        "    shutil.rmtree(root_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJmuoUIBNOFs"
      },
      "source": [
        "[<sup id=\"fn1\">1</sup>](#fn1-back) Acknowledgement: This usage is inspired by [Conditional with statement in Python](https://stackoverflow.com/a/68682614) by [Lucas Vasquez](https://stackoverflow.com/users/10712525/lucas-vazquez), used with adaptations under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/), accessed June 14, 2022."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}